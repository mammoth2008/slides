    <!doctype html>
    <html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=1024" />
        <meta name="apple-mobile-web-app-capable" content="yes" />

<!-- 修改页面标题 -->
        <title>ITA | HUST</title>

<!-- 修改 content -->
        <meta name="description" content="智能技术及应用课程使用, 为哈尔滨理工大学经济管理学院全日制硕士研究生设计. " />
        <meta name="author" content="xinjiang@hrbust.edu.cn" />

<!-- 引入数学公式 -->
        <script type="text/javascript">
            window.MathJax = {
              tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
              },
              chtml: {
                scale: 0.95
              }
            };
        </script>
        <script src="../extras/mathjax/tex-chtml-full.js"></script>
<!-- 引入完毕 -->

        <link href="../css/impress-course.css" rel="stylesheet" />
        <link href="../css/impress-common.css" rel="stylesheet" />

        <link rel="shortcut icon" href="../hustlogo72.png" />
        <link rel="stylesheet" href="../extras/highlight/styles/github.css">

    </head>

    <body class="impress-not-supported">

<!-- 左上角加上 logo -->
    <div class="logo"></div>

    <div class="fallback-message">
        <p>你的浏览器不支持本网站所要求的功能, 现在看到的是本网站简化版本.</p>
        <p>为了获得最佳体验, 请使用最新的Chrome, Safari, Firefox 或 Edge 浏览器.</p>

    </div>

    <div 
    id                       = "impress" 
    data-transition-duration = "1000" 
    data-width               = "1024" 
    data-height              = "768" 
    data-max-scale           = "3" 
    data-min-scale           = "0" 
    data-perspective         = "1000" 
    data-autoplay            = "0">

<!-- 每一节 slide 的标题是 cxyt", 表示章节标题. 如第一章第一节标题就是 c11t, 第三章第二节标题就是 c32t -->
<!-- 每一节 slide 的标题页 class 应该是 step, 不是 step markdown -->
<!-- 类似地, 需要 HTML 排版的 slide,class 都是 step; 可以使用 markdown 的, class 才是 step markdown -->

    <div 
    id         = "overview" 
    data-x     = "0" 
    data-y     = "0" 
    class      = "step" 
    data-scale = "10">

    </div>

<!-- 修改 id -->

    <div 
    id         = "c23t"
    class      = "step"
    data-x     = "0"
    data-y     = "0"
    data-z     = "0"
    data-scale = "5">

    <!-- 修改标题 -->
        <h2>2. 机器学习与深度学习</h2>
        <h3 style="text-align: center">2.3 深度学习的原理与应用</h3>
        <p class="footnote">
            <inlinecode style="font-size: 16px">
            Powered by 
            </inlinecode>
                <a href="https://github.com/impress/impress.js/">
                    <inlinecode style="font-size: 16px">
                    impress.js
                    </inlinecode>
                    <img class="icon" src="../favicon.png" style="width:10px;height:10px;">
                </a>
                <br/>
                <inlinecode style="font-size: 16px">
                    Ver. 2408
                </inlinecode>
            </inlinecode>
        </p>

    </div>
        
    <div 
    id            = "a1"
    class         = "step markdown"
    data-rel-to   = "c23t"
    data-rel-x    = "-2500"
    data-rel-y    = "-1600"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### Transformer 模型

- Transformer 是首个完全基于注意力机制的模型
- 用于处理序列数据
- 由 Vaswani 等人提出, 最初用于机器翻译任务
- 引领了 NLP 领域的一系列创新, 包括 BERT, GPT 等

    </div>

    <div 
    id            = "a2"
    class         = "step markdown"
    data-rel-to   = "a1"
    data-rel-x    = "0"
    data-rel-y    = "1200"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 自注意力机制

- 自注意力 (Self-Attention) 允许输入序列中的每个元素对其他所有元素进行加权
- $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V $$
- $Q, K, V$ 分别是查询 (Query) , 键 (Key) , 值 (Value) 矩阵, $d_k$ 是键向量的维度
- $ QK^T $ 是查询矩阵和键矩阵的点积操作, 用于计算查询与所有键之间的相似度或匹配程度

    </div>

    <div 
    id            = "a3"
    class         = "step markdown"
    data-rel-to   = "a2"
    data-rel-x    = "0"
    data-rel-y    = "1200"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 自注意力机制

- $ \frac{1}{\sqrt{d\_k}} $ 缩放点积得分, 避免梯度消失问题
- $ d\_k $ 是键向量的维度, 根据维度的根号来缩放
- $ \text{softmax} $ 函数将点积得分转换为概率分布形式
- 将 softmax 输出的注意力权重矩阵与值矩阵 $ V $ 相乘, 得到加权求和后的输出
- 输出被视为输入查询的加权表示

    </div>

    <div 
    id            = "a4"
    class         = "step markdown"
    data-rel-to   = "a3"
    data-rel-x    = "0"
    data-rel-y    = "1200"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 多头注意力机制

- 多头注意力是自注意力的一种扩展
- 将注意力机制并行化
- 每个 "头" 独立学习输入序列的不同方面
- $$ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}\_1, \dots, \text{head}\_h)W^O $$
- 其中, 每个 $ head\_i = Attention(QW\_i^Q, KW\_i^K, VW\_i^V) $

    </div>

    <div 
    id            = "a5"
    class         = "step markdown"
    data-rel-to   = "a4"
    data-rel-x    = "0"
    data-rel-y    = "1000"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 编码器与解码器

- Transformer 模型包含编码器和解码器各 N 层
- 编码器层包括多头注意力和前馈神经网络
- 解码器层在此基础上增加了掩码多头注意力, 以防止未来信息的泄露

    </div>

    <div 
    id            = "a6"
    class         = "step markdown"
    data-rel-to   = "c23t"
    data-rel-x    = "-200"
    data-rel-y    = "1200"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 位置编码

- Transformer 完全基于注意力机制
- 缺乏对序列顺序的内在理解
- 位置编码向模型注入关于元素位置的信息
- 使用正弦和余弦函数的组合进行编码
- $$ PE\_{(pos, 2i)} = \sin(pos/10000^{2i/d\_{\text{model}}}) $$
- $$ PE\_{(pos, 2i+1)} = \cos(pos/10000^{2i/d\_{\text{model}}}) $$

    </div>

    <div 
    id            = "a7"
    class         = "step markdown"
    data-rel-to   = "a6"
    data-rel-x    = "0"
    data-rel-y    = "800"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 使用正弦和余弦函数

- 周期性 (Periodicity)
- 不同频率
- 可推广性

    </div>

    <div 
    id            = "a8"
    class         = "step markdown"
    data-rel-to   = "a7"
    data-rel-x    = "0"
    data-rel-y    = "800"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 数学解释

- 使用 $ \frac{pos}{10000^{2i/d\_{\text{model}}}} $ 是让不同维度上的位置编码具有不同的频率
- $ 10000^{2i/d_{\text{model}}} $ 的底数是个大常数, 使得频率随维度 $ i $ 而显著变化

    </div>

    <div 
    id            = "a9"
    class         = "step markdown"
    data-rel-to   = "a8"
    data-rel-x    = "0"
    data-rel-y    = "800"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 层归一化与残差连接

- 残差连接帮助缓解深层网络中的梯度消失问题
- 层归一化 (Layer Normalization) 加快收敛速度
- 残差连接公式: $ \text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x)) $

    </div>

    <div 
    id            = "a10"
    class         = "step markdown"
    data-rel-to   = "c23t"
    data-rel-x    = "1600"
    data-rel-y    = "1200"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 训练策略与挑战

- Transformer 需要大量数据来有效训练
- 使用大量的 GPU/TPU 资源以及高效数据并行处理
- 主要挑战包括梯度消失, 过拟合以及训练成本

    </div>

    <div 
    id            = "a11"
    class         = "step markdown"
    data-rel-to   = "a10"
    data-rel-x    = "0"
    data-rel-y    = "800"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### Transformer 实际应用

- 广泛应用于语言理解, 文本生成, 摘要以及翻译等
- 在处理大规模数据集时表现卓越
- 引领了自然语言处理技术的新方向

    </div>

    <div 
    id            = "a12"
    class         = "step markdown"
    data-rel-to   = "a11"
    data-rel-x    = "0"
    data-rel-y    = "800"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### Diffusion 模型

- 传统生成模型: GAN 和 VAE
- Diffusion 模型基于过程生成
- 生成质量
- 多样性

    </div>

    <div 
    id            = "a13"
    class         = "step markdown"
    data-rel-to   = "a12"
    data-rel-x    = "0"
    data-rel-y    = "800"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### Diffusion 模型基本原理

- 正向过程: 逐步加入噪声
- 反向过程: 逐步从噪声中恢复数据
- 正向过程: $ p(\mathbf{x}\_{t-1} | \mathbf{x}\_t) $
- 反向过程: $ q(\mathbf{x}\_t | \mathbf{x}\_{t-1}) $

    </div>

    <div 
    id            = "a14"
    class         = "step markdown"
    data-rel-to   = "c23t"
    data-rel-x    = "3000"
    data-rel-y    = "-1600"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 正向过程

- 逐渐向数据中添加噪声的过程
- 从原始数据 $ \mathbf{x}\_0 $ 开始, 逐步引入噪声
- 最终生成近似噪声的数据 $ \mathbf{x}\_T $
- 条件概率分布, 其中 $ \mathbf{x}\_t $ 是在给定 $ \mathbf{x}\_{t-1} $ 的情况下生成
- $$ q(\mathbf{x}\_t | \mathbf{x}\_{t-1}) = \mathcal{N}(\mathbf{x}\_t; \sqrt{1-\beta\_t} \mathbf{x}\_{t-1}, \beta\_t \mathbf{I}) $$
- $ \beta\_t $ 决定了每一步加入的噪声量
- $ \mathcal{N} $ 表示高斯分布

    </div>

    <div 
    id            = "a15"
    class         = "step markdown"
    data-rel-to   = "a14"
    data-rel-x    = "0"
    data-rel-y    = "1200"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 反向过程

- 从噪声数据重构原始数据的过程
- 从噪声状态重建出干净的数据状态
- $$ p(\mathbf{x}\_{t-1} | \mathbf{x}\_t) = \mathcal{N}(\mathbf{x}\_{t-1}; \mu\_{\theta}(\mathbf{x}\_t, t), \sigma\_t^2 \mathbf{I}) $$
- $ \mu\_{\theta}(\mathbf{x}\_t, t) $ 是由神经网络参数化的函数, 预测给定 $ \mathbf{x}\_t $ 下 $ \mathbf{x}\_{t-1} $ 的最可能状态
- $ \sigma\_t^2 $ 是噪声的方差, 是固定值或者由模型动态预测

    </div>

    <div 
    id            = "a16"
    class         = "step markdown"
    data-rel-to   = "a15"
    data-rel-x    = "0"
    data-rel-y    = "1000"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 核心组件: 噪声添加与去除

- 添加噪声的策略和效果
- 去除噪声的机制
- 噪声模型对生成质量的影响

    </div>

    <div 
    id            = "a17"
    class         = "step markdown"
    data-rel-to   = "a16"
    data-rel-x    = "0"
    data-rel-y    = "1100"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### Diffusion 模型的训练数据要求

- 数据类型与质量要求
- 大量数据的处理与优化

    </div>

    <div 
    id            = "a18"
    class         = "step markdown"
    data-rel-to   = "a17"
    data-rel-x    = "0"
    data-rel-y    = "800"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 训练 Diffusion 模型的损失函数

- 损失函数: $ \mathcal{L} = \mathbb{E}[\|\mathbf{x}\_0 - \hat{\mathbf{x}}\_0(\mathbf{x}\_t)\|^2] $
- 如何优化模型以最小化重建误差
- 损失函数的影响: 模型稳定性与生成质量

    </div>

    <div 
    id            = "a19"
    class         = "step markdown"
    data-rel-to   = "a18"
    data-rel-x    = "0"
    data-rel-y    = "800"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 训练中的挑战与解决方案

- 训练资源需求: 计算力与时间
- 优化策略: 如何提高效率与减少资源消耗
- 技术进展: 利用新算法与硬件优化模型性能

    </div>

    <div 
    id            = "a20"
    class         = "step markdown"
    data-rel-to   = "c23t"
    data-rel-x    = "4800"
    data-rel-y    = "-1600"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 时间步的调整方法与重要性

- 时间步数量的选择如何影响模型性能
- 时间步的细微调整对结果的具体影响
- 实践中的最佳实践与常见误区

    </div>

    <div 
    id            = "a21"
    class         = "step markdown"
    data-rel-to   = "a20"
    data-rel-x    = "0"
    data-rel-y    = "600"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 条件生成

- 条件生成的概念与应用
- 如何在 Diffusion 模型中实施条件生成

    </div>

    <div 
    id            = "a22"
    class         = "step markdown"
    data-rel-to   = "a21"
    data-rel-x    = "0"
    data-rel-y    = "600"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### Diffusion 模型的技术创新

- Denoising Diffusion Probabilistic Models
- DDPM 的工作原理与技术优势
- DDPM 与其他 Diffusion 模型的比较

    </div>

    <div 
    id            = "a23"
    class         = "step markdown"
    data-rel-to   = "a22"
    data-rel-x    = "0"
    data-rel-y    = "800"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### DDPM

- DDPM 是一种基于随机过程的生成模型
- 模拟物理世界中的扩散过程
- 在高质量图像生成, 音频合成等领域表现出色

    </div>

    <div 
    id            = "a24"
    class         = "step markdown"
    data-rel-to   = "a23"
    data-rel-x    = "0"
    data-rel-y    = "800"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### DDPM 的工作原理

- 噪声模型: $ q(\mathbf{x}\_{t} | \mathbf{x}\_{t-1}) = \mathcal{N}(\mathbf{x}\_t; \sqrt{1-\beta\_t} \mathbf{x}\_{t-1}, \beta\_t \mathbf{I}) $
- 反向过程: $ p\_\theta(\mathbf{x}\_{t-1} | \mathbf{x}\_t) = \mathcal{N}(\mathbf{x}\_{t-1}; \mu\_\theta(\mathbf{x}\_t, t), \sigma\_t^2 \mathbf{I}) $
- 使用神经网络预测去噪过程中每一步的参数, 如均值 $ \mu\_\theta $ 和方差 $ \sigma\_t^2 $

    </div>

    <div 
    id            = "a25"
    class         = "step markdown"
    data-rel-to   = "a24"
    data-rel-x    = "0"
    data-rel-y    = "800"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### DDPM 的技术优势

- 高质量生成
- 稳定性
- 可控性

    </div>

    <div 
    id            = "a26"
    class         = "step markdown"
    data-rel-to   = "a25"
    data-rel-x    = "0"
    data-rel-y    = "800"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### DDPM 与其他 Diffusion 模型的比较

- DDPM 通过预测噪声来优化反向过程, 效率更高
- 生成高保真度图像时更稳定, 避免 GAN 的训练困难
- 提供了一种不同于 VAE 的视角

    </div>

    <div 
    id            = "a27"
    class         = "step markdown"
    data-rel-to   = "a26"
    data-rel-x    = "0"
    data-rel-y    = "800"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### Improved Diffusion

- Improved Diffusion 的关键改进点
- 性能提升的具体实例
- 如何实现更高效的 Diffusion 模型

    </div>

    <div 
    id            = "a28"
    class         = "step markdown"
    data-rel-to   = "c23t"
    data-rel-x    = "-2400"
    data-rel-y    = "-2800"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 性能与效率

- 针对传统模型的性能瓶颈提出的改进版本
- 减少所需的采样步骤, 提高生成速度和质量
- 使用更复杂的网络结构
- 改进训练技巧如使用残差连接和注意力机制

    </div>

    <div 
    id            = "a29"
    class         = "step markdown"
    data-rel-to   = "a28"
    data-rel-x    = "2000"
    data-rel-y    = "0"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 性能提升

- 通过减少采样步骤, 从数千步降低到数百步
- 更细致的图像细节和更高的图像分辨率
- 在任务如超分辨率和条件图像生成中展现显著改进

    </div>

    <div 
    id            = "a30"
    class         = "step markdown"
    data-rel-to   = "a29"
    data-rel-x    = "2000"
    data-rel-y    = "0"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 实现更高效的 Diffusion 模型

- 采用分层训练方法, 逐层优化网络性能
- 引入高效网络设计, 如深度可分卷积和多尺度结构
- 使用先进的算法加速如 FFT 加速的卷积运算, 优化内存和计算资源的使用

    </div>

    <div 
    id            = "a31"
    class         = "step markdown"
    data-rel-to   = "a30"
    data-rel-x    = "2000"
    data-rel-y    = "0"
    data-rel-z    = "0"
    data-rotate-y = "0"
    data-scale    = "2">

### 最新进展与未来方向

- Diffusion 模型在各领域的新应用
- 技术发展的趋势与挑战
- 未来研究方向与潜在的突破

    </div>

    <div
    id            = "mm23"
    class         = "step markdown"
    data-rel-to   = "c23t"
    data-rel-x    = "0"
    data-rel-y    = "-2000"
    data-rotate-y = "0"
    data-rotate   = "0"
    data-scale    = "1">

![course 2.3 mindmap](img/c02/mindmap-2-3.png)

    </div>

<!-- 本节问题 -->

    <div
    id            = "c23q"
    class         = "step markdown"
    data-rel-to   = "c23t"
    data-rel-x    = "0"
    data-rel-y    = "-800"
    data-z        = "500"
    data-rotate-x = "77"
    data-rotate-y = "0"
    data-rotate-z = "180"
    data-scale    = "2">

### 2.3 深度学习的原理与应用

- 解释什么是自注意力机制.
- 讨论自注意力在 Transformer 模型中的重要性.
- 为什么 Diffusion 模型生成过程具有高稳定性?
- 尝试做一个实验, 使用 DDPM 生成特定风格的图像.
- 讨论使用注意力机制的优势, 特别在处理大数据时.
- 思考一种可能的场景, 将 Transformer 和 Diffusion 模型结合使用, 以解决现实世界中的问题.

----

[ 2.2 多层神经网络的诞生与发展](ita-2-2.html#/overview)
[| 练习 |](ita-exec.html)
[ 2.4 深度学习的现状与不足](ita-2-4.html#/overview)

    </div>

    </div>

    <!--
    <div id="impress-toolbar"></div>
    <div id="impress-help"></div>

    <script>
    if ("ontouchstart" in document.documentElement) { 
        document.querySelector(".hint").innerHTML = "<p>Swipe left or right to navigate</p>";
    }
    </script>
    -->

    <!-- 页面下方显示进度条 -->
    <div class="impress-progressbar">
        <div></div>
    </div>

    <!-- 页面下方显示当前页/总页数 -->
    <div class="impress-progress"></div>

    <!-- 使用 markdown 写简单页面 -->
    <script type="text/javascript" src="../extras/markdown/markdown.js"></script>

    <!-- 语法高亮,配合<head>中的 highlight css 起作用 -->
    <script src="../extras/highlight/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>

    <!-- impress.js 需要放在最下面,并且初始化 -->
    <script src="../js/impress.js"></script>
    <script>impress().init();</script>
    </body>
    </html>
